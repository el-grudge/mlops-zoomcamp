In week 5ï¸âƒ£ of the MLOps Zoomcamp we learned about

ğŸ”ğŸ¤– Monitoring ML Models in Production 

* â³ğŸ› ï¸ Performance of ML models in production degrades with time, and the right monitoring will allow us to intervene in time.
* First, monitor service health with metrics like:
    - â±ï¸ uptime
    - ğŸ§  memory usage 
    - ğŸ•’ latency
* Second, monitor model health with metrics like: 
    - ğŸ“Šâœ”ï¸ model accuracy (choice of metric depends on the problem, whether it's a ranking (ex. Normalized Discounted Cumulative Gain), classification (ex. Log Loss), or regression (ex. mean absolute error))
    - âš–ï¸ğŸ¥ğŸ’° model bias, especially in highly critical domains such health care or finance
    - ğŸ“‰ underperforming segments 
    - ğŸ“ˆâš ï¸ outliers, especially when the cost of each individual error is high
    - ğŸ—£ï¸ğŸ” explainability
* Third, monitor data health with metrics like: 
    - ğŸ·ï¸ğŸ“‰ data quality and integriy (ex. categorical values proportions, missing / null values)
    - ğŸš§ğŸš« broken pipelines / data outages, 
    - ğŸ“‹ğŸ”„ schema change, 
    - ğŸ”„ğŸ“Š data drift (the statistical properties of the input data change over time) , 
    - ğŸ”„ğŸ“‰ concept drift (the statistical properties of the target variable, or the relationship between the input features and the target variable change over time)

* How to monitor?
    - ğŸ› ï¸ğŸ“ˆ if you already have some monitoring architecture for service health, you can leverage those to monitor your ml model (ex. Prometheus / Grafana)
    - ğŸ“ŠğŸ–¥ï¸ use a bi tool to create a dashboard to monitor your model (ex. tableau / looker)

* ğŸŒŠğŸ—‚ï¸ Monitoring non-batch models (ex. a streaming / online model) can be more complicated, especially when calculating things like data drift and concept drift. The best way to tackle this issue is to use a window function (ex. moving window with / without moving reference) to break up the data into batches and compare those windows.

Emeli Dral showed us how to setup a monitoring service using evidently.ai and Grafana. Here's a simple guide:
1- ğŸ–¥ï¸ Created a virtual environvment
2- ğŸğŸ“¦ Install python libraries
3- ğŸ³âš™ï¸ Create a docker compose yml to run the services that will be needed for the monitoring application (postgres, adminer, grafana)
4- ğŸ“ŠğŸ”§ Use evidently.ai metrics to monitor column drift, dataset drift, dataset missing values, column quantiles, dataset correlations, and much more (you can learn more about the available metrics [here](https://docs.evidentlyai.com/reference/all-metrics#data-quality))

ğŸ“ğŸ” Monitoring a production model
In the weekly project we built a monitoring dashboard using evidently and Grafana and looked at multiple metrics from our trip duration model, including the fare amount 50th quantile. You can view the code [here](https://github.com/el-grudge/mlops-zoomcamp/tree/main/week_5).

#mlops_zoomcamp #monitoring #evidently #grafana #machine_learning #docker #learning_in_public
